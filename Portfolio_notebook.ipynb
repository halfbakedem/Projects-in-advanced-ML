{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Github Repo\n",
        "\n",
        "This notebook serves as an explanatory document detailing the three projects I've worked on this term."
      ],
      "metadata": {
        "id": "zVKmjnHfOfOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 1: UN World Happiness data report\n",
        "\n",
        " [Link to project 1](https://github.com/halfbakedem/Projects-in-advanced-ML/tree/main/Project%201)\n",
        "\n",
        "The U.N. World Happiness Data is a dataset that contains annual survey results of the level of happiness and well-being in different countries. The data includes various factors that contribute to happiness, such as economic production, social support, life expectancy, freedom to make life choices, generosity, and perceptions of corruption. The dataset is compiled and published by the United Nations Sustainable Development Solutions Network, and it covers over 150 countries from 2012 to present.\n",
        "\n"
      ],
      "metadata": {
        "id": "jb2iH6IROt5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project description:\n",
        "\n",
        "I ran three initial models:\n",
        "1. Random forest classifier\n",
        "2. Gradient Boosting classifier\n",
        "3. 3-layer Neural Network\n",
        "\n",
        "These models performed relatively well on the leadership board and relative to my teams on slack. Particularly, my NN model was the best performing model of all my team.\n",
        "\n",
        "Nonetheless, I attempted to attain even better results by adopting new hyperparameters from my teammates\n",
        "\n",
        "1. This did not improve my Random Forest model\n",
        "2. This improved my gradient boost modeel\n",
        "3. None of the team attempted a NN model; I increased the epochs and batch size further to stretch the model. However, this did not improve model score likely due to overfitting on a relatively simple dataset"
      ],
      "metadata": {
        "id": "iQ2KUasjQ-Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 2: Covid X-Ray data\n",
        "\n",
        " [Link to project 2](https://github.com/halfbakedem/Projects-in-advanced-ML/tree/main/Project%202)\n",
        "\n",
        "This dataset is a collection of chest X-ray and CT scan images of COVID-19 patients as well as patients with other respiratory diseases such as SARS, MERS, and ARDS. The dataset was created by compiling images from various sources, including public repositories and medical publications.\n",
        "\n",
        "The dataset consists of a total of 13,975 images, with 3,910 images labeled as COVID-19 positive, 5,248 images labeled as normal, and the rest labeled as other respiratory diseases. The images were preprocessed and resized to a fixed dimension of 224 x 224 pixels before being used for training the deep learning model."
      ],
      "metadata": {
        "id": "DT4lzlfuSFmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project description: \n",
        "\n",
        "I ran three initial models:\n",
        "\n",
        "1. Tensor flow CNN with 8 layers and smaller filters\n",
        "2. Tensor flow CNN with 6 layers and larger filters\n",
        "3. Transfer learning using the Inception V3\n",
        "\n",
        "All three models performed quite well on the leardership board and relative to teammates, attaining at least 75% on the f1-score. The first model was the poorest performing and the transfer learning model the best.\n",
        "\n",
        "I decided to fit 2 more models given that the first three performed very well relative to my teammates.\n",
        "1. I increased the number of layers and epochs, as advised by teammates, for my CNN. This improved my f1-score and accuracy to oveer 81% each, making it the best model thus far.\n",
        "2. I ran an ever larger model of 2 extra layers and 5 epochs. This performed ever better, with an f1-score and accuracy of about 83% each."
      ],
      "metadata": {
        "id": "Y7-B5QCHSTGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 3: Stanford SST Sentiment Dataset:\n",
        "\n",
        "[Link to project 3](https://github.com/halfbakedem/Projects-in-advanced-ML/tree/main/Project%203)\n",
        "\n",
        "The Stanford SST Sentiment Dataset is a collection of movie reviews labeled with their sentiment polarity (positive or negative). The dataset was created for the purpose of training and evaluating machine learning models in the domain of sentiment analysis. The dataset consists of 11,855 sentences extracted from movie reviews and annotated with binary sentiment labels. The dataset was created by the"
      ],
      "metadata": {
        "id": "9JgoKt_pT3Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Projecet description\n",
        "\n",
        "I ran three initial models\n",
        "1. Using an embedding and Conv1d layer\n",
        "2. Using an embedding and LSTM layer\n",
        "3. Using transfer learning with glove embeddings\n",
        "\n",
        "My models were rather poor, achieving scores in the 50-60+% range. Of the three, the third model performed best with an accuracy and f1-score of roughly 68% each.\n",
        "\n",
        "Learning from my teammates, I decided to\n",
        "\n",
        "1. Increase number of epochs for the Conv1d model\n",
        "2. Decrease my first LSTM layer to 32 units\n",
        "3. Add another dense layer to the transfer learning model\n",
        "\n",
        "This substantially improved results for the first two models. The first model now had an f1-score and accuracy of about 79%, the second 79% and 80%. However, the transfer learning model only improved very slightly to 69% on both metrics. "
      ],
      "metadata": {
        "id": "G8H4q7jOT9kb"
      }
    }
  ]
}